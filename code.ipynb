{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFrvgk5ccoeI"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"naserabdullahalam/phishing-email-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4a44ff8"
      },
      "source": [
        "import os\n",
        "\n",
        "# list all files and directories in given path\n",
        "all_files = os.listdir(path)\n",
        "\n",
        "# filter for CSV files\n",
        "csv_files = [f for f in all_files if f.endswith('.csv')]\n",
        "\n",
        "print(\"CSV files in the dataset directory:\")\n",
        "for csv_file in csv_files:\n",
        "    print(csv_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# List to store individual dataframes\n",
        "dfs = []\n",
        "\n",
        "# Iterate through the list of CSV files\n",
        "for file_name in csv_files:\n",
        "    file_path = os.path.join(path, file_name)\n",
        "    try:\n",
        "        # Read the CSV file, handling potential encoding or parsing errors\n",
        "        df = pd.read_csv(file_path, on_bad_lines='skip', encoding_errors='replace')\n",
        "\n",
        "        # Check if the required columns exist\n",
        "        if 'body' in df.columns and 'label' in df.columns:\n",
        "            # Extract only the needed columns\n",
        "            subset = df[['body', 'label']]\n",
        "            dfs.append(subset)\n",
        "            print(f\"Processed {file_name}: {len(subset)} rows added.\")\n",
        "        else:\n",
        "            print(f\"Skipping {file_name}: Missing 'body' or 'label' columns.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_name}: {e}\")\n",
        "\n",
        "# combine all dataframes\n",
        "if dfs:\n",
        "    combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # save dataset\n",
        "    output_file = 'combined_email_dataset.csv'\n",
        "    combined_df.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"\\nSuccessfully saved combined dataset to '{output_file}'\")\n",
        "    print(f\"Total rows: {len(combined_df)}\")\n",
        "else:\n",
        "    print(\"No data found to combine.\")"
      ],
      "metadata": {
        "id": "ZxLqvfLkisjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('combined_email_dataset.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9h3yEp59kdWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# preparing data\n",
        "print(f'Total samples before cleaning: {len(df)}')\n",
        "df = df.dropna(subset=['body', 'label']).copy()\n",
        "\n",
        "# Ensure labels are integers (0 or 1)\n",
        "df['label'] = pd.to_numeric(df['label'], errors='coerce').astype(int)\n",
        "\n",
        "# Extract sentences and labels as lists\n",
        "sentences = df.body.values\n",
        "labels = df.label.values\n",
        "\n",
        "print(f'Total samples after cleaning: {len(df)}')"
      ],
      "metadata": {
        "id": "UXRfOFlQ61NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading tokenizer and checking max length of tokens from sample\n",
        "# print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "lengths = [len(tokenizer.tokenize(t)) for t in df.body[:1000]]\n",
        "print(max(lengths))  # shows that there are email bodies with more tokens than BERT's max of 512\n"
      ],
      "metadata": {
        "id": "VdDI9TEH_f5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in sentences:\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "      str(sent),\n",
        "      add_special_tokens=True,\n",
        "      max_length=128, # will not be able to use the entire email body\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "      truncation=True\n",
        "  )\n",
        "\n",
        "  # adding encoded sentence and attention mask to their respective lists\n",
        "  input_ids.append(encoded_dict['input_ids'])\n",
        "  attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "\n",
        "# convert to tensors\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n"
      ],
      "metadata": {
        "id": "UOutZrb8-BQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training split and dataloaders\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# 80-10-10 train val split\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "generator = torch.Generator().manual_seed(42)  # to keep seed consistent\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size], generator=generator)\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "print('{:>5,} test samples'.format(test_size))\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# create dataloaders for both sets\n",
        "# training samples in random order for variety in training\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset), # batches taken sequentially\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            sampler = SequentialSampler(test_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "print('data setup done')"
      ],
      "metadata": {
        "id": "mgJC3U0eKugi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "# defining helper functions\n",
        "def flat_accuracy(preds, labels):\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "  '''\n",
        "  take time in seconds and return a string hh:mm:ss\n",
        "  '''\n",
        "\n",
        "  # first round to nearest second\n",
        "  elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "fl7bm4vTuU3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "import torch\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# defining model and optimizer\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "# linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# run on GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.cuda()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8\n",
        "                )"
      ],
      "metadata": {
        "id": "GsKs1sD-16E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# training\n",
        "epochs = 3\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs  # [number of batches] x [number of epochs]\n",
        "\n",
        "# learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=total_steps\n",
        "                                            )\n",
        "\n",
        "# training loop\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()  # measure duration of epoch\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # update progress every 40 batches\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input_ids\n",
        "        #   [1]: attention_masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # clear previously calculated gradients before backward pass\n",
        "        model.zero_grad()\n",
        "\n",
        "        # forward pass (evaluate the model on this training batch)\n",
        "        result = model(b_input_ids,\n",
        "                       token_type_ids=None,\n",
        "                       attention_mask=b_input_mask,\n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        # accumulate training loss over all batches to\n",
        "        # calculate average loss at the end\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # perform backward pass to calculate gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # clip the norm of the gradients to 1.0.\n",
        "        # to prevent \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # update parameters and take a step using computed gradient\n",
        "        optimizer.step()\n",
        "\n",
        "        # update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "    # calculate average loss over all batches\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "\n",
        "    # measure how long epoch took\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "    # VALIDATION\n",
        "\n",
        "    # after each training epoch, measure performance on\n",
        "    # validation set\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training)\n",
        "        with torch.no_grad():\n",
        "            result = model(b_input_ids,\n",
        "                           token_type_ids=None,\n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        # accumulate validation loss\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences and\n",
        "        # accumulate over all batches\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "\n",
        "    # report final accuracy for validation run\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # calculate avg loss over all batches\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    # duration of validation\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # stats from epoch\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "metadata": {
        "id": "f5gTfFs93wsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST SET EVALUATION\n",
        "\n",
        "print(\"Running Evaluation on Test Set...\")\n",
        "\n",
        "t0 = time.time()\n",
        "model.eval() # put model in evaluation mode\n",
        "\n",
        "# tracking variables\n",
        "total_test_accuracy = 0\n",
        "total_test_loss = 0\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# predict\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    # add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # no gradients\n",
        "    with torch.no_grad():\n",
        "        # forward pass, calculate logit predictions\n",
        "        result = model(b_input_ids,\n",
        "                       token_type_ids=None,\n",
        "                       attention_mask=b_input_mask,\n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "    logits = result.logits\n",
        "    loss = result.loss\n",
        "    total_test_loss += loss.item()\n",
        "\n",
        "    # move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # store predictions and true labels\n",
        "    total_test_accuracy += flat_accuracy(logits, label_ids)\n",
        "    predictions.extend(np.argmax(logits, axis=1).flatten())\n",
        "    true_labels.extend(label_ids.flatten())\n",
        "\n",
        "# final accuracy for test run\n",
        "avg_test_accuracy = total_test_accuracy / len(test_dataloader)\n",
        "print(\"  Test Accuracy: {0:.2f}\".format(avg_test_accuracy))\n",
        "print(\"  Test Loss: {0:.2f}\".format(total_test_loss / len(test_dataloader)))\n",
        "print(\"  Evaluation took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edIzZP9PBG9j",
        "outputId": "b7cdf2bf-ed27-4c57-a5be-05c61cea849e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Evaluation on Test Set...\n",
            "  Test Accuracy: 0.99\n",
            "  Test Loss: 0.04\n",
            "  Evaluation took: 0:00:26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# helper fxn to gather all predictions\n",
        "def get_all_predictions(model, dataloader):\n",
        "    model.eval() # model to evaluation mode\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(\"Running inference to calculate metrics...\")\n",
        "\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        # move batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # get model outputs (logits)\n",
        "        with torch.no_grad():\n",
        "            result = model(b_input_ids,\n",
        "                           token_type_ids=None,\n",
        "                           attention_mask=b_input_mask,\n",
        "                           return_dict=True)\n",
        "\n",
        "        logits = result.logits\n",
        "\n",
        "        # logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # logits to class predictions (0 or 1)\n",
        "        batch_preds = np.argmax(logits, axis=1)\n",
        "\n",
        "        # add to lists\n",
        "        all_preds.extend(batch_preds)\n",
        "        all_labels.extend(label_ids)\n",
        "\n",
        "    return all_labels, all_preds\n",
        "\n",
        "\n",
        "true_labels, predicted_labels = get_all_predictions(model, test_dataloader)\n",
        "\n",
        "# Print the Standard Metrics Report\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"FINAL EVALUATION REPORT\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Calculate simple accuracy\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Overall Accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "# target_names assumes 0 is Legitimate and 1 is Phishing/Spam\n",
        "print(\"\\nDetailed Metrics:\")\n",
        "print(classification_report(true_labels, predicted_labels, target_names=['Legitimate', 'Spam']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Rp-mIJFKbWW",
        "outputId": "e55307d1-5244-44af-e010-e34857c33e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference to calculate metrics...\n",
            "\n",
            "==============================\n",
            "FINAL EVALUATION REPORT\n",
            "==============================\n",
            "Overall Accuracy: 99.28%\n",
            "\n",
            "Detailed Metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Legitimate       0.99      0.99      0.99      3947\n",
            "        Spam       0.99      0.99      0.99      4302\n",
            "\n",
            "    accuracy                           0.99      8249\n",
            "   macro avg       0.99      0.99      0.99      8249\n",
            "weighted avg       0.99      0.99      0.99      8249\n",
            "\n"
          ]
        }
      ]
    }
  ]
}